{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d025f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a099a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params(m):\n",
    "    return sum([p.numel() for p in m.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed90b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Attention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.seq_len\n",
    "        \n",
    "        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "        \n",
    "        self.register_buffer('mask',torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n",
    "        \n",
    "        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.residual_dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        b,t,c = x.shape\n",
    "        # q,k,v shape individually: batch_size x seq_len x embed_dim\n",
    "        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n",
    "        q,k,v = self.c_attn(x).chunk(3,dim=-1)\n",
    "        q = rearrange(q,'b t (h n) -> b n t h',n=self.n_heads) # h = head_size\n",
    "        k = rearrange(k,'b t (h n) -> b n t h',n=self.n_heads)\n",
    "        v = rearrange(v,'b t (h n) -> b n t h',n=self.n_heads)\n",
    "        \n",
    "        # qk_t = einsum(q,k,'b n t1 h, b n t2 h -> b n t1 t2') * self.scale\n",
    "        qk_t = (q@k.transpose(-2,-1)) * self.scale\n",
    "        # fun fact, limit mask to [:,:,:t,:t] else short prompts will not work\n",
    "        qk_t = qk_t.masked_fill(self.mask==0,float('-inf'))\n",
    "        qk_t = F.softmax(qk_t,dim=-1)\n",
    "        \n",
    "        weights = self.attn_dropout(qk_t)\n",
    "        \n",
    "        attention = weights @ v # batch x n_heads x seq_len x head_size\n",
    "        attention = rearrange(attention,'b n t h -> b t (n h)') # batch x n_heads x seq_len x embed_dim\n",
    "        \n",
    "        out = self.c_proj(attention)\n",
    "        out = self.resid_dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e90a759d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(vocab_size=50257,\n",
       "          embed_dim=768,\n",
       "          num_heads=12,\n",
       "          seq_len=1024,\n",
       "          depth=12,\n",
       "          attention_dropout=0.1,\n",
       "          residual_dropout=0.1,\n",
       "          mlp_ratio=4,\n",
       "          mlp_dropout=0.1,\n",
       "          emb_dropout=0.1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = SimpleNamespace(\n",
    "    vocab_size = 50_257,\n",
    "    embed_dim = 768,\n",
    "    num_heads = 12,\n",
    "    seq_len = 1024,\n",
    "    depth = 12,\n",
    "    attention_dropout = 0.1,\n",
    "    residual_dropout = 0.1,\n",
    "    mlp_ratio = 4,\n",
    "    mlp_dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    ") # gpt2 small\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82abaec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024, 768]), 2362368)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = GPT2Attention(config)\n",
    "x = torch.rand(1,config.seq_len,config.embed_dim)\n",
    "x.shape, params(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf733ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['mask', 'c_attn.weight', 'c_attn.bias', 'c_proj.weight', 'c_proj.bias'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46c60a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "  (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16d6a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.mlp_ratio = config.mlp_ratio\n",
    "        self.mlp_dropout = config.mlp_dropout\n",
    "        \n",
    "        self.c_fc = nn.Linear(self.embed_dim,self.embed_dim*self.mlp_ratio)\n",
    "        self.c_proj = nn.Linear(self.embed_dim*self.mlp_ratio,self.embed_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(self.mlp_dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05ffcfeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['c_fc.weight', 'c_fc.bias', 'c_proj.weight', 'c_proj.bias'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = GPT2MLP(config)\n",
    "mlp.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71a4af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.ln_1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.attn = GPT2Attention(config)\n",
    "        self.ln_2 = nn.LayerNorm(self.embed_dim)\n",
    "        self.mlp = GPT2MLP(config)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x+self.attn(self.ln_1(x))\n",
    "        x = x+self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2443c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Block(\n",
      "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (attn): GPT2Attention(\n",
      "    (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "    (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (mlp): GPT2MLP(\n",
      "    (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "    (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['ln_1.weight', 'ln_1.bias', 'attn.mask', 'attn.c_attn.weight', 'attn.c_attn.bias', 'attn.c_proj.weight', 'attn.c_proj.bias', 'ln_2.weight', 'ln_2.bias', 'mlp.c_fc.weight', 'mlp.c_fc.bias', 'mlp.c_proj.weight', 'mlp.c_proj.bias'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = GPT2Block(config)\n",
    "print(block)\n",
    "block.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6a5f8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n",
    "            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n",
    "            drop = nn.Dropout(config.emb_dropout),\n",
    "            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n",
    "            ln_f = nn.LayerNorm(config.embed_dim)\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        token_embeddings = self.transformer.wte(x) # batch x seq_len\n",
    "        pos_embs = torch.arange(0,x.size(1)).to(x.device)\n",
    "        positional_embeddings = self.transformer.wpe(pos_embs)\n",
    "        x = self.transformer.drop(token_embeddings+positional_embeddings)\n",
    "        for h in self.transformer.h:\n",
    "            x = h(x) # batch_size x seq_len x embed_dim\n",
    "        x = self.transformer.ln_f(x)[:,[-1],:] # get last hidden state: batch_size x 1 x embed_dim\n",
    "        x = self.lm_head(x) # batch_size x vocab_size\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self,idx,max_new_tokens=5,temperature=1.0):\n",
    "        \n",
    "        for _ in range(max_new_tokens+1):\n",
    "            \n",
    "            inp = idx if idx.size(1) <= self.config.seq_len else inp[:,-self.config.seq_len:]\n",
    "            out = self(inp)\n",
    "            out = out[:, -1, :] / temperature\n",
    "            probs = F.softmax(out, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            \n",
    "        return idx[:,-max_new_tokens:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7e68a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2Model(config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cc664a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fea7fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,config.seq_len).long()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7f9f6248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50257])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a80829d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0,config.vocab_size,(1,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d042f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = gpt2.generate(x)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d7537a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[38890,  9988, 11832, 30680, 33973]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e08e8740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7894,  7780, 16557, 37860, 30901]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "59618995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(gpt2.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ddbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "001cb74685912b83d95c1a0b04a11963ca377cd35d659c9726095075375e23e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
