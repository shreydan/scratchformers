{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181123bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "from einops import einsum, rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4752784b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(embed_dim=128,\n",
       "          num_heads=4,\n",
       "          depth=6,\n",
       "          pool='mean',\n",
       "          img_size=224,\n",
       "          num_channels=3,\n",
       "          patch_size=16,\n",
       "          attention_dropout=0.0,\n",
       "          residual_dropout=0.0,\n",
       "          mlp_ratio=4,\n",
       "          mlp_dropout=0.0,\n",
       "          pos_dropout=0.0,\n",
       "          num_classes=1000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_vit_pico = SimpleNamespace(\n",
    "    embed_dim = 128,\n",
    "    num_heads = 4,\n",
    "    depth = 6,\n",
    "    pool = 'mean',\n",
    "    img_size = 224,\n",
    "    num_channels = 3,\n",
    "    patch_size = 16,\n",
    "    attention_dropout = 0.,\n",
    "    residual_dropout = 0.,\n",
    "    mlp_ratio = 4,\n",
    "    mlp_dropout = 0.,\n",
    "    pos_dropout = 0.,\n",
    "    num_classes = 1000\n",
    ")\n",
    "\n",
    "config_vit_pico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67c59ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params(m):\n",
    "    return sum([p.numel() for p in m.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a4b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.n_heads = config.num_heads\n",
    "        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n",
    "        self.head_size = self.embed_dim // self.n_heads\n",
    "        self.seq_len = config.embed_dim\n",
    "        \n",
    "        self.qkv = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=False)\n",
    "        self.scale = self.head_size ** -0.5\n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(config.attention_dropout)\n",
    "        \n",
    "        self.proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.residual_dropout = nn.Dropout(config.residual_dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        b,t,c = x.shape\n",
    "        # q,k,v shape individually: batch_size x seq_len x embed_dim\n",
    "        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n",
    "        q,k,v = self.qkv(x).chunk(3,dim=-1)\n",
    "        q = rearrange(q,'b t (h n) -> b n t h',n=self.n_heads) # h = head_size\n",
    "        k = rearrange(k,'b t (h n) -> b n t h',n=self.n_heads)\n",
    "        v = rearrange(v,'b t (h n) -> b n t h',n=self.n_heads)\n",
    "        \n",
    "        # qk_t = einsum(q,k,'b n t1 h, b n t2 h -> b n t1 t2') * self.scale\n",
    "        qk_t = (q@k.transpose(-2,-1)) * self.scale\n",
    "        \n",
    "        weights = self.attention_dropout(qk_t)\n",
    "        \n",
    "        attention = weights @ v # batch x n_heads x seq_len x head_size\n",
    "        attention = rearrange(attention,'b n t h -> b t (n h)') # batch x n_heads x seq_len x embed_dim\n",
    "        \n",
    "        out = self.proj(attention)\n",
    "        out = self.residual_dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92aceb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 128])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(16,config_vit_pico.embed_dim,config_vit_pico.embed_dim)\n",
    "a = MultiHeadAttention(config_vit_pico)\n",
    "a(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db10faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.embed_dim,config.embed_dim*config.mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.embed_dim*config.mlp_ratio,config.embed_dim),\n",
    "            nn.Dropout(config.mlp_dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x+self.attn(self.ln1(x))\n",
    "        x = x+self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0785db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        config.num_patches = (config.img_size // config.patch_size) ** 2\n",
    "        config.patch_dim = config.num_channels * config.patch_size ** 2\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            nn.LayerNorm(self.config.patch_dim),\n",
    "            nn.Linear(self.config.patch_dim, self.config.embed_dim, bias=False),\n",
    "            nn.LayerNorm(self.config.embed_dim)\n",
    "        )\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1,self.config.num_patches,self.config.embed_dim),requires_grad=True)\n",
    "        self.pos_dropout = nn.Dropout(self.config.pos_dropout)\n",
    "        \n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(config) for _ in range(self.config.depth)])\n",
    "        \n",
    "        self.head = nn.Linear(self.config.embed_dim,self.config.num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = rearrange(x,'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "                      p1=self.config.patch_size,\n",
    "                      p2=self.config.patch_size\n",
    "                     )\n",
    "        x = self.patch_embedding(x)\n",
    "        x += self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = x.mean(dim=1)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0db2a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1000]),\n",
       " 1440744,\n",
       " namespace(embed_dim=128,\n",
       "           num_heads=4,\n",
       "           depth=6,\n",
       "           pool='mean',\n",
       "           img_size=224,\n",
       "           num_channels=3,\n",
       "           patch_size=16,\n",
       "           attention_dropout=0.0,\n",
       "           residual_dropout=0.0,\n",
       "           mlp_ratio=4,\n",
       "           mlp_dropout=0.0,\n",
       "           pos_dropout=0.0,\n",
       "           num_classes=1000,\n",
       "           num_patches=196,\n",
       "           patch_dim=768))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViT(config_vit_pico)\n",
    "x = torch.rand(1,3,224,224)\n",
    "model(x).shape, params(model), model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2aeeecc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (patch_embedding): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=768, out_features=128, bias=False)\n",
       "    (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-5): 6 x TransformerBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (residual_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=128, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f7bd477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1000]), 86530024)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_vit_base = SimpleNamespace(\n",
    "    embed_dim = 768,\n",
    "    num_heads = 12,\n",
    "    depth = 12,\n",
    "    img_size = 224,\n",
    "    num_channels = 3,\n",
    "    patch_size = 16,\n",
    "    attention_dropout = 0.,\n",
    "    residual_dropout = 0.,\n",
    "    mlp_ratio = 4,\n",
    "    mlp_dropout = 0.,\n",
    "    pos_dropout = 0.,\n",
    "    num_classes = 1000\n",
    ")\n",
    "\n",
    "base = ViT(config_vit_base)\n",
    "x = torch.rand(1,3,224,224)\n",
    "base(x).shape, params(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b0e8d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (patch_embedding): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): MultiHeadAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (residual_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (3): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b447074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9628,  0.7251,  0.7515,  ..., -0.0577,  2.6292,  0.7296],\n",
       "         [ 0.8384, -0.2621,  0.1058,  ..., -0.9684,  0.2455,  0.1826],\n",
       "         [ 0.5375,  0.0921, -0.1405,  ..., -0.2842, -0.2453,  1.3140],\n",
       "         ...,\n",
       "         [-0.4933,  0.2625,  1.3823,  ...,  1.2591, -0.4266, -0.3067],\n",
       "         [ 1.0388,  0.2339, -0.1040,  ..., -1.0064, -1.0018, -0.6355],\n",
       "         [-1.6927,  1.6512,  1.0339,  ...,  0.3604, -0.8847, -0.6002]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3523897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
