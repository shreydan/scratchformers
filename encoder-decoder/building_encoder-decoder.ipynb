{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e37e96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "07b81e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        assert dim % n_heads == 0, 'dim should be div by n_heads'\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.q = nn.Linear(dim,dim)\n",
    "        self.k = nn.Linear(dim,dim)\n",
    "        self.v = nn.Linear(dim,dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.out_proj = nn.Linear(dim,dim)\n",
    "        \n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        b,t,c = q.shape\n",
    "        q = self.q(q)\n",
    "        k = self.k(k)\n",
    "        v = self.v(v)\n",
    "        q = q.view(b,t,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        k = k.view(b,t,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        v = v.view(b,t,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        qkT = torch.matmul(q,k.transpose(-1,-2)) * self.scale\n",
    "        qkT = self.attn_dropout(qkT)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.to(dtype=qkT.dtype,device=qkT.device)\n",
    "            qkT = qkT.masked_fill(mask==0,float('-inf'))\n",
    "            \n",
    "        qkT = F.softmax(qkT,dim=-1)\n",
    "            \n",
    "        attn = torch.matmul(qkT,v)\n",
    "        attn = attn.permute(0,2,1,3).contiguous().view(b,t,c)\n",
    "        out = self.out_proj(attn)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6556b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,dim,dropout=0.):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim,dim*4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim*4,dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9d7902f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, attn_dropout=0., mlp_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadAttention(dim,n_heads,attn_dropout)\n",
    "        self.ffd = FeedForward(dim,mlp_dropout)\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self,x,mask=None):\n",
    "        x = self.ln_1(x)\n",
    "        x = x + self.attn(x,x,x,mask)\n",
    "        x = self.ln_2(x)\n",
    "        x = x + self.ffd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "52d46f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, attn_dropout=0., mlp_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(dim,n_heads,attn_dropout)\n",
    "        self.cross_attn = MultiheadAttention(dim,n_heads,attn_dropout)\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.ln_3 = nn.LayerNorm(dim)\n",
    "        self.ffd = FeedForward(dim,mlp_dropout)\n",
    "        \n",
    "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
    "        x = self.ln_1(x)\n",
    "        x = x + self.self_attn(x,x,x,tgt_mask)\n",
    "        x = self.ln_2(x)\n",
    "        x = x + self.cross_attn(x,enc_out,enc_out,src_mask) # decoder: q, encoder: k,v\n",
    "        x = self.ln_3(x)\n",
    "        x = x + self.ffd(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8b687969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,max_len,dim):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.class_embedding = nn.Embedding(vocab_size,dim)\n",
    "        self.pos_embedding = nn.Embedding(max_len,dim)\n",
    "    def forward(self,x):\n",
    "        x = self.class_embedding(x)\n",
    "        pos = torch.arange(0,self.max_len,device=x.device)\n",
    "        x = x + self.pos_embedding(pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7e050a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_embedding = Embedding(config['encoder_vocab_size'],config['encoder_max_len'],config['dim'])\n",
    "        self.dec_embedding = Embedding(config['decoder_vocab_size'],config['decoder_max_len'],config['dim'])\n",
    "        \n",
    "        self.depth = config['depth']\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(\n",
    "                dim=config['dim'],\n",
    "                n_heads=config['n_heads'],\n",
    "                attn_dropout=config['attn_dropout'],\n",
    "                mlp_dropout=config['mlp_dropout']\n",
    "            ) for _ in range(self.depth)\n",
    "        ])\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                dim=config['dim'],\n",
    "                n_heads=config['n_heads'],\n",
    "                attn_dropout=config['attn_dropout'],\n",
    "                mlp_dropout=config['mlp_dropout']\n",
    "            ) for _ in range(self.depth)\n",
    "        ])\n",
    "        \n",
    "        self.src_pad_token_id = config['src_pad_token_id']\n",
    "        self.register_buffer('tgt_mask',torch.tril(torch.ones(1,1,config['decoder_max_len'],config['decoder_max_len'])))\n",
    "    \n",
    "    def create_src_mask(self,src):\n",
    "        return (src != self.src_pad_token_id).unsqueeze(1).unsqueeze(2) # N, 1, 1, src_len\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \n",
    "        src_mask = self.create_src_mask(src)\n",
    "        \n",
    "        enc_out = self.enc_embedding(src)\n",
    "        dec_out = self.dec_embedding(tgt)\n",
    "        \n",
    "        \n",
    "        for i in range(self.depth):\n",
    "            enc_out = self.encoders[i](enc_out,mask=src_mask)\n",
    "            dec_out = self.decoders[i](dec_out,enc_out,src_mask=src_mask,tgt_mask=self.tgt_mask)\n",
    "            \n",
    "                \n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c64705a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dim': 512,\n",
    "    'n_heads': 8,\n",
    "    'attn_dropout': 0.1,\n",
    "    'mlp_dropout': 0.1,\n",
    "    'depth': 6,\n",
    "    'encoder_vocab_size': 20_000,\n",
    "    'encoder_max_len': 128,\n",
    "    'decoder_vocab_size': 25_000,\n",
    "    'decoder_max_len': 128,\n",
    "    'src_pad_token_id': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "812ba188",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e46646e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67309568"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "69f8afa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (enc_embedding): Embedding(\n",
       "    (class_embedding): Embedding(20000, 512)\n",
       "    (pos_embedding): Embedding(128, 512)\n",
       "  )\n",
       "  (dec_embedding): Embedding(\n",
       "    (class_embedding): Embedding(25000, 512)\n",
       "    (pos_embedding): Embedding(128, 512)\n",
       "  )\n",
       "  (encoders): ModuleList(\n",
       "    (0-5): 6 x EncoderBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ffd): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0-5): 6 x DecoderBlock(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiheadAttention(\n",
       "        (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffd): FeedForward(\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): Dropout(p=0.1, inplace=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a245cf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128]), torch.Size([1, 128]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.randint(0,config['encoder_vocab_size'],size=(1,config['encoder_max_len']))\n",
    "tgt = torch.randint(0,config['decoder_vocab_size'],size=(1,config['decoder_max_len']))\n",
    "src.shape, tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dd963b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 512])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src,tgt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4cdb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
