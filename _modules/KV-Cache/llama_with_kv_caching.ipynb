{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4ac4e576-b7d2-446b-9f52-1334001f5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1a407664-b3f8-4f8b-ac07-b3229ac753e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99e0a66d-84b3-4c33-bb1f-59f32b047ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.ones(self.embed_dim,dtype=torch.float32),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x [B, S, D]\n",
    "        mean = x.pow(2).mean(dim=-1,keepdim=True)\n",
    "        r_sqrt = x * torch.rsqrt(mean + 1e-5) # [B, S, 1]\n",
    "        y = r_sqrt * self.weight\n",
    "        return y.to(x.dtype)\n",
    "\n",
    "\n",
    "class SiLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # x [B S D]\n",
    "        return x * F.sigmoid(x)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.intermediate_dim = config.intermediate_dim\n",
    "        self.gate_proj = nn.Linear(self.embed_dim, self.intermediate_dim, bias=False, dtype=config.dtype)\n",
    "        self.up_proj = nn.Linear(self.embed_dim, self.intermediate_dim, bias=False, dtype=config.dtype)\n",
    "        self.down_proj = nn.Linear(self.intermediate_dim, self.embed_dim, bias=False, dtype=config.dtype)\n",
    "        self.act_fn = SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x [B S D]\n",
    "        x1 = self.gate_proj(x)\n",
    "        x2 = self.up_proj(x)\n",
    "        x = self.act_fn(x1) * x2\n",
    "        x = self.down_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baece363-0d9d-47e7-b7c3-b0c3c131a8db",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0a89ec4e-e8f9-4670-a66e-ebacba5df2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope(head_dim, base_theta=10_000, context_length=4096):\n",
    "    k = torch.arange(0,head_dim,2)[:head_dim//2].float()\n",
    "    inv_freq = 1 / (base_theta ** (k/head_dim))\n",
    "\n",
    "    positions = torch.arange(context_length)\n",
    "    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0) # [S, H/2]\n",
    "    angles = torch.cat([angles, angles],dim=-1) # [S, H]\n",
    "\n",
    "    cos = torch.cos(angles) # [S, H]\n",
    "    sin = torch.sin(angles) # [S, H]\n",
    "\n",
    "\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e8722ff3-121b-4b6c-bbb1-b1063b080edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x, cos, sin, positions=None):\n",
    "    B, nH, S, H = x.shape\n",
    "    x1 = x[...,:H//2] # [B, nH, S, H/2]\n",
    "    x2 = x[...,H//2:] # [B, nH, S, H/2]\n",
    "    if positions is None:\n",
    "        positions = torch.arange(S)\n",
    "    cos_values = cos[positions,:].unsqueeze(0).unsqueeze(1) # [1,1,S,H]\n",
    "    sin_values = sin[positions,:].unsqueeze(0).unsqueeze(1) # [1,1,S,H]\n",
    "    rotated = torch.cat([-x2,x1],dim=-1)\n",
    "    x_rope = (x * cos_values) + (rotated * sin_values)\n",
    "    return x_rope.to(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf64ff-b90c-464a-9176-d4992ce32691",
   "metadata": {},
   "source": [
    "# KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5a504437-49ab-4c94-8f87-64fb386f423f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3fd74150-aabc-4a7b-90a9-54211b21f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, max_length, head_dim, n_heads, dtype=torch.float32, device='cpu'):\n",
    "        self.max_length = max_length\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.keys = torch.zeros((1, self.n_heads, self.max_length, self.head_dim),\n",
    "                                device=self.device, dtype=self.dtype)\n",
    "        self.values = torch.zeros((1, self.n_heads, self.max_length, self.head_dim),\n",
    "                                  device=self.device, dtype=self.dtype)\n",
    "        self.length = 0\n",
    "\n",
    "    def update(self, new_key, new_value):\n",
    "        # new_key/new_value: [B, n_heads, S, head_dim] (with B==1 during inference)\n",
    "        S = new_key.shape[2]\n",
    "        assert self.length + S <= self.max_length, \"KV cache overflow\"\n",
    "        seq_start = self.length\n",
    "        seq_end = seq_start + S\n",
    "        self.keys[:, :, seq_start:seq_end, :] = new_key\n",
    "        self.values[:, :, seq_start:seq_end, :] = new_value\n",
    "        self.length = seq_end\n",
    "\n",
    "    def get(self):\n",
    "        if self.length == 0:\n",
    "            return None, None\n",
    "        return self.keys[:, :, :self.length, :], self.values[:, :, :self.length, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "50bc5e6e-0315-46a4-b01e-1429df58a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kvcache = KVCache(100,32,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c3cb27dc-c650-4b46-ab21-aa3df09c12c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 100, 32])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvcache.keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "43c09dad-9c23-4062-ab77-29ff82935257",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.rand(1,4,10,32)\n",
    "v = torch.rand(1,4,10,32)\n",
    "kvcache.update(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f80360fc-cbb6-471c-95a1-b8146fe5289b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvcache.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4e68a423-ad81-42d1-a12a-3d5d8d064643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 10, 32]), torch.Size([1, 4, 10, 32]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_k, past_v = kvcache.get()\n",
    "past_k.shape, past_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d9888ac3-52c6-4b59-9489-e1ceacec8f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(k,past_k), torch.allclose(v,past_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2e699064-3b4d-42ab-b5f5-f16a30c66b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(kvcache.length,kvcache.length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "581788fb-75b6-493f-aadf-bc28b175a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_k = torch.rand(1,4,1,32)\n",
    "new_v = torch.rand(1,4,1,32)\n",
    "kvcache.update(new_k,new_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "76788e0f-2791-4b56-a490-942aa09cc45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 11, 32]), torch.Size([1, 4, 11, 32]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_k, past_v = kvcache.get()\n",
    "past_k.shape, past_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "76772a45-84f6-4782-bc3a-29898987b4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 11, 32]), torch.Size([1, 4, 11, 32]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_k = torch.cat([k,new_k],dim=2)\n",
    "full_v = torch.cat([v,new_v],dim=2)\n",
    "full_k.shape, full_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c4bb632c-9684-470e-8a15-2fe51092fdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(past_k,full_k), torch.allclose(past_v,full_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9ec2ccca-0ec2-4734-b7c6-26eb5a70266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos, sin = precompute_rope(32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ffe00cfe-e2c5-40bd-8051-4b0ff1b3a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "rope_k = apply_rope(full_k, cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d237a5fb-8b30-430e-beaa-71c45de52ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 11, 32])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rope_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "db5198ac-d807-43d9-87da-3f60d6ef510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_k_rope = apply_rope(new_k, cos, sin, positions=torch.arange(kvcache.length,kvcache.length+1)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9c022c25-1c61-4abe-bd63-a4f9938c45a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1, 32])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k_rope.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9beadc70-f97f-4162-a313-2460025f1a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(rope_k[:,:,[-1],:],new_k_rope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2d8e8e19-9bf1-4811-8da2-62b1567b591e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kvcache.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791749e1-1604-4984-9f14-e776f531bc05",
   "metadata": {},
   "source": [
    "# Integrating it into GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ec5b1d99-72c5-441f-bedb-bdfc3e16e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = config.embed_dim\n",
    "        self.num_kv_heads = config.num_kv_heads\n",
    "        self.num_q_heads = config.num_q_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "\n",
    "        assert self.embed_dim % self.num_q_heads == 0, \"embed_dim must be divisible by num_q_heads\"\n",
    "        assert self.num_q_heads % self.num_kv_heads == 0, \"num_q_heads must be divisible by num_kv_heads\"\n",
    "\n",
    "        self.head_dim = self.embed_dim // self.num_q_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.head_dim * self.num_q_heads, bias=False, dtype=config.dtype)\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.head_dim * self.num_kv_heads, bias=False, dtype=config.dtype)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.head_dim * self.num_kv_heads, bias=False, dtype=config.dtype)\n",
    "\n",
    "        self.drop = nn.Dropout(config.attn_dropout)\n",
    "        self.o_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False, dtype=config.dtype)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            torch.triu(torch.ones(config.max_position_embeddings, config.max_position_embeddings), diagonal=1)\n",
    "        )\n",
    "\n",
    "        cos, sin = precompute_rope(self.head_dim, base_theta=config.base_theta,  # Important: RoPE applies to half dimension\n",
    "                                  context_length=self.max_position_embeddings)\n",
    "        self.register_buffer(\"rope_cos\", cos)\n",
    "        self.register_buffer(\"rope_sin\", sin)\n",
    "\n",
    "        self.kv_cache = None\n",
    "        self.use_cache = False\n",
    "\n",
    "    def enable_kv_cache(self, dtype=None):\n",
    "        self.kv_cache = KVCache(self.max_position_embeddings, self.head_dim, self.num_kv_heads, dtype, self.rope_cos.device)\n",
    "        self.use_cache = True\n",
    "    \n",
    "    def reset_kv_cache(self):\n",
    "        if self.kv_cache is not None:\n",
    "            self.kv_cache.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, S, D]\n",
    "        B, S, D = x.shape\n",
    "\n",
    "        q = self.q_proj(x) # [B S H*nQ]\n",
    "        k = self.k_proj(x) # [B S H*nKV]\n",
    "        v = self.v_proj(x) # [B S H*nKV]\n",
    "\n",
    "        q = q.view(B, S, self.num_q_heads, self.head_dim).transpose(1,2) # [B nQ S H]\n",
    "        k = k.view(B, S, self.num_kv_heads, self.head_dim).transpose(1,2) # [B nKV S H]\n",
    "        v = v.view(B, S, self.num_kv_heads, self.head_dim).transpose(1,2) # [B nKV S H]\n",
    "\n",
    "        if self.use_cache and self.kv_cache is not None:\n",
    "            assert B == 1, \"Batch size must be 1 in inference when using KV cache.\"\n",
    "            past_length = self.kv_cache.length\n",
    "            positions = torch.arange(past_length, past_length + S, device=x.device)\n",
    "        else:\n",
    "            positions = torch.arange(0, S, device=x.device)\n",
    "\n",
    "        # Apply RoPE\n",
    "        q = apply_rope(q, self.rope_cos, self.rope_sin, positions)\n",
    "        k = apply_rope(k, self.rope_cos, self.rope_sin, positions)\n",
    "\n",
    "        if self.use_cache and self.kv_cache is not None:\n",
    "            self.kv_cache.update(k, v)\n",
    "            k, v = self.kv_cache.get()\n",
    "            total_length = k.shape[2]\n",
    "        else:\n",
    "            total_length = S\n",
    "\n",
    "        k = k.repeat_interleave(self.num_q_heads//self.num_kv_heads, dim=1) # [B nQ S H]\n",
    "        v = v.repeat_interleave(self.num_q_heads//self.num_kv_heads, dim=1) # [B nQ S H]\n",
    "\n",
    "        attn = q @ k.transpose(2,3) # [B nQ S1 H] @ [B nQ H S2] = [B nQ S1 S2]\n",
    "        \n",
    "        if self.use_cache and self.kv_cache is not None:\n",
    "            mask = self.causal_mask[past_length:past_length+S, :total_length].bool()\n",
    "        else:\n",
    "            mask = self.causal_mask[:S, :S].bool()\n",
    "            \n",
    "        attn.masked_fill_(mask, -torch.inf)\n",
    "        \n",
    "        attn = F.softmax(attn / (self.head_dim ** 0.5), dim=-1)\n",
    "\n",
    "        attn = self.drop(attn)\n",
    "\n",
    "        out = attn @ v # [B nQ S S] @ [B nQ S H] = [B nQ S H]\n",
    "        out = out.transpose(1,2) # [B S nQ H]\n",
    "        out = out.reshape(B, S, D)\n",
    "\n",
    "        proj = self.o_proj(out)\n",
    "        \n",
    "        return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c1048624-bfcd-4304-9903-5cab22cf6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = GroupedQueryAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "        \n",
    "        self.input_layernorm = LlamaRMSNorm(config.embed_dim)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.embed_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x [B S D]\n",
    "        skip = x\n",
    "        x = self.input_layernorm(x)\n",
    "        x = self.self_attn(x)\n",
    "        x = x + skip\n",
    "        \n",
    "        skip = x\n",
    "        x = self.post_attention_layernorm(x)\n",
    "        x = self.mlp(x)\n",
    "        x = x + skip\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(\n",
    "            self.config.vocab_size, \n",
    "            self.config.embed_dim, \n",
    "            padding_idx=self.config.eos_token_id,\n",
    "            dtype=self.config.dtype)\n",
    "        self.layers = nn.ModuleList([\n",
    "            LlamaDecoderLayer(self.config) for _ in range(self.config.num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = LlamaRMSNorm(self.config.embed_dim)\n",
    "        self.lm_head = nn.Linear(self.config.embed_dim, self.config.vocab_size, bias=False, dtype=self.config.dtype)\n",
    "\n",
    "        self._tie_weights()\n",
    "\n",
    "    def _tie_weights(self):\n",
    "        self.lm_head.weight = self.embed_tokens.weight\n",
    "\n",
    "    def enable_kv_cache(self):\n",
    "        for layer in self.layers:\n",
    "            layer.self_attn.enable_kv_cache(dtype=self.config.dtype)\n",
    "\n",
    "    def reset_kv_cache(self):\n",
    "        for layer in self.layers:\n",
    "            layer.self_attn.kv_cache.reset()\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids [B S]\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "99df9b8c-769c-4353-9f20-b989b9070de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    embed_dim = 576,\n",
    "    intermediate_dim = 1536,\n",
    "    max_position_embeddings = 8192,\n",
    "    base_theta = 100000,\n",
    "    num_q_heads = 9,\n",
    "    num_kv_heads = 3,\n",
    "    attn_dropout = 0.,\n",
    "    num_layers = 30,\n",
    "    vocab_size = 49152,\n",
    "    dtype = torch.bfloat16,\n",
    "    eos_token_id = 2\n",
    "    )\n",
    "model = LLaMA(config)\n",
    "model.eval()\n",
    "model.enable_kv_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80f289-21cb-4042-8857-6e03c1159c44",
   "metadata": {},
   "source": [
    "# Inference with KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c11d3533-c81b-485d-ada6-4a3035f461be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./simpleVLM')\n",
    "smol = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "\n",
    "smol_sd = smol.state_dict()\n",
    "model_sd = model.state_dict()\n",
    "smol_sd = {k:v for k,v in smol_sd.items() if not any([s in k for s in ['rope','causal_mask']])}\n",
    "\n",
    "for smol_key,smol_value in smol_sd.items():\n",
    "    model_key = smol_key.replace('model.','')\n",
    "    model_sd[model_key] = smol_value.clone()\n",
    "\n",
    "model.load_state_dict(model_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "167c2ede-c6a2-4022-9088-0cfb19ceac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(text):\n",
    "    messages = [{\"role\": \"user\", \"content\": text}]\n",
    "    input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bdc3c1ef-bb96-4fd8-8379-3e4ad6dca3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    config,\n",
    "    input_ids,\n",
    "    max_new_tokens=32,\n",
    "    temperature=0.0,\n",
    "):\n",
    "    model.eval()\n",
    "    \n",
    "    context_length = model.config.max_position_embeddings\n",
    "    eos_token_id = model.config.eos_token_id\n",
    "    \n",
    "    model.reset_kv_cache()\n",
    "    \n",
    "    inputs = input_ids.clone()\n",
    "    if inputs.shape[1] > context_length:\n",
    "        inputs = inputs[:, -context_length:]\n",
    "    print(tokenizer.decode(inputs.flatten().tolist()))\n",
    "    \n",
    "    # Prefill\n",
    "    with torch.inference_mode():\n",
    "        _ = model(inputs)\n",
    "    \n",
    "    all_tokens = inputs\n",
    "    for token_idx in range(max_new_tokens):\n",
    "        with torch.inference_mode():\n",
    "            last_token = all_tokens[:, [-1]]\n",
    "            logits = model(last_token)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            if temperature > 0.:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "                probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            if next_token.item() == eos_token_id:\n",
    "                break\n",
    "            \n",
    "            print(tokenizer.decode(next_token.flatten().tolist()), end='')\n",
    "            all_tokens = torch.cat([all_tokens, next_token], dim=1)\n",
    "    print()\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c19381bb-2976-412c-bd9a-b5ec14541258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "give me a random fact about llamas<|im_end|>\n",
      "\n",
      "llama* is a large, four-legged mammal native to the Andes Mountains of South America. They are known for their long, muscular legs and powerful legs, which are used for jumping and running. Llamas are herbivores, and they eat a diet of grasses, leaves, and other vegetation. They are also known for their ability to climb trees, which they use to reach high branches and reach food sources. Llamas are also known for their unique ability to produce a strong, sticky mucus that helps them to climb and move through the Andes.\n"
     ]
    }
   ],
   "source": [
    "inputs = get_input('give me a random fact about llamas')\n",
    "generated = generate(model, config, inputs, max_new_tokens=250, temperature=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b657998b-241c-4433-b28e-2a3000300f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "can you do function calling?<|im_end|>\n",
      "\n",
      "<|im_start|>assistant\n",
      " here's a simple function that calls another function:\n",
      "\n",
      "```python\n",
      "function(func):\n",
      " func()urn\n",
      "\n",
      "(): main\n",
      "function(lambda: print(\"Hello\"))\n",
      "\n",
      "name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "call_function` is a function that takes a lambda function as an argument. The lambda function is called by `call_function` and returns the result of calling `call_function` with the lambda function as an argument.\n",
      "\n",
      "When you run this code, it will print \"Hello\". The `lambda` function is a special function in Python that is used to define a small, one-time-use function.\n",
      "\n",
      " `call_function` is a function, not a method. It's a way to call a function without creating an instance of the class.\n"
     ]
    }
   ],
   "source": [
    "inputs = get_input('can you do function calling?')\n",
    "generated = generate(model, config, inputs, max_new_tokens=250, temperature=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee81d471-a3ac-4cbf-a8f2-564b756132da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
