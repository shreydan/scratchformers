{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47e8e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange,repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e2b20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q,k,v,is_causal=False):\n",
    "    \"\"\"\n",
    "    q: b x n x t1 x hd\n",
    "    k,v: b x n x t2 x hd\n",
    "    qkT: b x n x t1 x t2\n",
    "    attention: b x n x t1 x d\n",
    "    \"\"\"\n",
    "    b,d = q.size(0),q.size(2)\n",
    "    scale = 1 / q.size(2) ** 0.5\n",
    "    t1 = q.size(1)\n",
    "    t2 = k.size(1)\n",
    "    \n",
    "    qkT = q @ k.transpose(-1,-2) * scale\n",
    "    \n",
    "    if is_causal:\n",
    "        mask = torch.tril(torch.ones_like(qkT)).to(device=qkT.device)\n",
    "        qkT = qkT.masked_fill(mask==0,float('-inf'))\n",
    "        \n",
    "    qkT = qkT.softmax(dim=-1)\n",
    "        \n",
    "    attention = qkT @ v\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf8b6a2",
   "metadata": {},
   "source": [
    "# Multi Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a686cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, is_causal = False, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dim % n_heads == 0, 'dim should be div by num heads'\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.is_causal = is_causal\n",
    "        \n",
    "        self.qkv = nn.Linear(self.dim,self.dim*3,bias=qkv_bias)\n",
    "        self.proj = nn.Linear(self.dim, self.dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        b,s,d = x.size()\n",
    "        \n",
    "        qkv = self.qkv(x).chunk(3,dim=-1)\n",
    "        \n",
    "        q,k,v = map(lambda t: t.view(b,s,self.n_heads,self.head_dim).permute(0,2,1,3),qkv)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(q,k,v,is_causal=self.is_causal)\n",
    "        attention = attention.permute(0,2,1,3).contiguous().view(b,s,d)\n",
    "        \n",
    "        return self.proj(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4fff2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(512,8,is_causal=True)\n",
    "mha(torch.rand(1,128,512)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f31f32",
   "metadata": {},
   "source": [
    "# Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45a6b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, q_bias=False, kv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dim % n_heads == 0, 'dim should be div by num heads'\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        \n",
    "        self.q = nn.Linear(self.dim,self.dim,bias=q_bias)\n",
    "        self.kv = nn.Linear(self.dim,self.dim*2,bias=kv_bias)\n",
    "        self.proj = nn.Linear(self.dim, self.dim)\n",
    "        \n",
    "    def forward(self, decoder_out, encoder_out):\n",
    "        \n",
    "        b,s,d = decoder_out.size()\n",
    "        \n",
    "        q = self.q(decoder_out)\n",
    "        k,v = self.kv(encoder_out).chunk(2,dim=-1)\n",
    "        \n",
    "        q = q.view(b,q.size(1),self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        k = k.view(b,k.size(1),self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        v = v.view(b,v.size(1),self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(q,k,v,is_causal=False)\n",
    "        attention = attention.permute(0,2,1,3).contiguous().view(b,s,d)\n",
    "        \n",
    "        return self.proj(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee585ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_out = torch.rand(1,128,512)\n",
    "enc_out = torch.rand(1,100,512)\n",
    "mhca = MultiHeadCrossAttention(512,8)\n",
    "mhca(dec_out,enc_out).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6c99f",
   "metadata": {},
   "source": [
    "# Grouped Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39efccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self,dim,n_heads,n_groups):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dim % n_heads == 0, 'dim should be div by n_heads'\n",
    "        assert n_heads % n_groups == 0, 'n_heads should be div by n_groups'\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.n_groups = n_groups\n",
    "        self.n_repeats = self.n_heads // self.n_groups\n",
    "        \n",
    "        self.q = nn.Linear(self.dim,self.head_dim*self.n_heads)\n",
    "        self.kv = nn.Linear(self.dim,self.head_dim*self.n_groups*2)\n",
    "        self.proj = nn.Linear(self.dim, self.dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        b,s,d = x.shape\n",
    "        \n",
    "        q = self.q(x)\n",
    "        k,v = self.kv(x).chunk(2,dim=-1)\n",
    "        \n",
    "        q = q.view(b,s,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        k = k.view(b,s,self.n_groups,self.head_dim).permute(0,2,1,3)\n",
    "        v = v.view(b,s,self.n_groups,self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        # repeat interleave: [1,2] * 3 => [1,1,1,2,2,2]\n",
    "        # k,v: b x n_groups x s x h => b x n_heads x s x h\n",
    "        # b x n_groups x 1 x s x h => b x n_groups x n_repeats x s x h => b x n_heads x s x h\n",
    "        k = k[:,:,None,:,:].expand(b, self.n_groups, self.n_repeats, s, self.head_dim).reshape(b, self.n_heads, s, self.head_dim)\n",
    "        v = v[:,:,None,:,:].expand(b, self.n_groups, self.n_repeats, s, self.head_dim).reshape(b, self.n_heads, s, self.head_dim)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(q,k,v,is_causal=True)\n",
    "        attention = attention.permute(0,2,1,3).contiguous().view(b,s,d)\n",
    "        \n",
    "        return self.proj(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fd261e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,12,16)\n",
    "gqa = GroupedQueryAttention(16,8,4)\n",
    "gqa(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9611bc6",
   "metadata": {},
   "source": [
    "### einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0088b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GQA(nn.Module):\n",
    "    def __init__(self,dim,n_heads,n_groups):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dim % n_heads == 0, 'dim should be div by n_heads'\n",
    "        assert n_heads % n_groups == 0, 'n_heads should be div by n_groups'\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.n_groups = n_groups\n",
    "        self.n_repeats = self.n_heads // self.n_groups\n",
    "        \n",
    "        self.q = nn.Linear(self.dim,self.head_dim*self.n_heads)\n",
    "        self.kv = nn.Linear(self.dim,self.head_dim*self.n_groups*2)\n",
    "        self.proj = nn.Linear(self.dim, self.dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        b,s,d = x.shape\n",
    "        \n",
    "        q = self.q(x)\n",
    "        k,v = self.kv(x).chunk(2,dim=-1)\n",
    "        \n",
    "        q = rearrange(q,'b s (n h) -> b n s h',n=self.n_heads, h=self.head_dim)\n",
    "        k = rearrange(k,'b s (g h) -> b g s h',g=self.n_groups, h=self.head_dim)\n",
    "        v = rearrange(v,'b s (g h) -> b g s h',g=self.n_groups, h=self.head_dim)\n",
    "        \n",
    "        k = repeat(k,'b g s h -> b (g r) s h',r=self.n_repeats)\n",
    "        v = repeat(v,'b g s h -> b (g r) s h',r=self.n_repeats)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(q,k,v,is_causal=True)\n",
    "        attention = rearrange(attention,'b n s h -> b s (n h)')\n",
    "        \n",
    "        return self.proj(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d90dcad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(1,12,16)\n",
    "gqa = GQA(16,8,4)\n",
    "gqa(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1f05bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
