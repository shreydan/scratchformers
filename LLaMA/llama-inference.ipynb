{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"aff495ac","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom types import SimpleNamespace\nfrom collections import OrderedDict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:34:12.257232Z","iopub.execute_input":"2024-11-17T18:34:12.258759Z","iopub.status.idle":"2024-11-17T18:34:12.264405Z","shell.execute_reply.started":"2024-11-17T18:34:12.258702Z","shell.execute_reply":"2024-11-17T18:34:12.263067Z"}},"outputs":[],"execution_count":23},{"id":"74e401bc","cell_type":"markdown","source":"#### a starting point: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb","metadata":{}},{"id":"dd25909e","cell_type":"markdown","source":"# RMSNorm\nreplaces LayerNorm\n\n$$\ny_i = \\frac{x_i}{\\text{RMS}(x)}\\gamma_i\\\\\nRMS(x) = \\sqrt{\\epsilon + \\frac{1}{n} \\sum x_i^2}\n$$\n$\\gamma$ is a learnable parameter\n- $x$ is input, $x_i$ will be one feature/neuron\ni.e. if x is of shape (1,128,1024) # bs, seq_len, num_fts then we need to normalize along the last dim of 1024 neurons\n- we init gamma with 1","metadata":{}},{"id":"277a7f08","cell_type":"code","source":"class LlamaRMSNorm(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.weight = nn.Parameter(\n            torch.ones(self.embed_dim,dtype=torch.float32),\n            requires_grad=True\n        )\n        \n    def forward(self, x):\n        # x [B, S, D]\n        mean = x.pow(2).mean(dim=-1,keepdim=True)\n        r_sqrt = x * torch.rsqrt(mean + 1e-5) # [B, S, 1]\n        y = r_sqrt * self.weight\n        return y.to(x.dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.098221Z","iopub.execute_input":"2024-11-17T18:22:49.098725Z","iopub.status.idle":"2024-11-17T18:22:49.108178Z","shell.execute_reply.started":"2024-11-17T18:22:49.098684Z","shell.execute_reply":"2024-11-17T18:22:49.106422Z"}},"outputs":[],"execution_count":2},{"id":"0517e8e8","cell_type":"code","source":"# testing RMSNorm\nrms = LlamaRMSNorm(embed_dim=8)\nx = torch.rand(1,3,8)\nrms(x).shape,torch.allclose(rms(x),nn.RMSNorm(8,eps=1e-5)(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.110338Z","iopub.execute_input":"2024-11-17T18:22:49.110929Z","iopub.status.idle":"2024-11-17T18:22:49.224479Z","shell.execute_reply.started":"2024-11-17T18:22:49.110798Z","shell.execute_reply":"2024-11-17T18:22:49.223351Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1, 3, 8]), True)"},"metadata":{}}],"execution_count":3},{"id":"729ac427","cell_type":"markdown","source":"# New activation function: SiLU (Swish)\n\n$$\\text{SiLU}(x) = x * \\sigma(x)$$\n$\\sigma(x)$ is sigmoid function ","metadata":{}},{"id":"3980d7f8","cell_type":"code","source":"class SiLU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, x):\n        # x [B S D]\n        return x * F.sigmoid(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.226986Z","iopub.execute_input":"2024-11-17T18:22:49.227392Z","iopub.status.idle":"2024-11-17T18:22:49.233640Z","shell.execute_reply.started":"2024-11-17T18:22:49.227353Z","shell.execute_reply":"2024-11-17T18:22:49.232464Z"}},"outputs":[],"execution_count":4},{"id":"2673be1f","cell_type":"code","source":"# testing SiLU\ntorch.allclose(SiLU()(x),F.silu(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.235253Z","iopub.execute_input":"2024-11-17T18:22:49.235729Z","iopub.status.idle":"2024-11-17T18:22:49.254462Z","shell.execute_reply.started":"2024-11-17T18:22:49.235675Z","shell.execute_reply":"2024-11-17T18:22:49.253202Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"id":"c0dd0d26","cell_type":"markdown","source":"# New MLP! SwiGLU\n\n$$\n\\text{SwiGLU}(x) = \\text{SiLU}(\\text{linear}_1(x)) * \\text{linear}_2(x) \\\\\n\\text{output} = \\text{linear}_3(\\text{SwiGLU}(x))\n$$","metadata":{}},{"id":"083e3748","cell_type":"code","source":"class LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.intermediate_dim = config.intermediate_dim\n        self.gate_proj = nn.Linear(self.embed_dim, self.intermediate_dim, bias=False, dtype=config.dtype)\n        self.up_proj = nn.Linear(self.embed_dim, self.intermediate_dim, bias=False, dtype=config.dtype)\n        self.down_proj = nn.Linear(self.intermediate_dim, self.embed_dim, bias=False, dtype=config.dtype)\n        self.act_fn = SiLU()\n        \n    def forward(self, x):\n        # x [B S D]\n        x1 = self.gate_proj(x)\n        x2 = self.up_proj(x)\n        x = self.act_fn(x1) * x2\n        x = self.down_proj(x)\n        return x\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.256178Z","iopub.execute_input":"2024-11-17T18:22:49.256658Z","iopub.status.idle":"2024-11-17T18:22:49.269301Z","shell.execute_reply.started":"2024-11-17T18:22:49.256605Z","shell.execute_reply":"2024-11-17T18:22:49.268111Z"}},"outputs":[],"execution_count":6},{"id":"7f8c736c","cell_type":"markdown","source":"# RoPE\nrotary positional embeddings!\n\n- applied to q,k at MHA step\n- precomputed angles, their sine and cosine based on model's context length\n- current implementation input shape: B, nH, S, H","metadata":{}},{"id":"a83658a9","cell_type":"code","source":"def precompute_rope(head_dim, base_theta=10_000, context_length=4096):\n    k = torch.arange(0,head_dim,2)[:head_dim//2].float()\n    inv_freq = 1 / (base_theta ** (k/head_dim))\n    \n    positions = torch.arange(context_length)\n    angles = positions.unsqueeze(1) * inv_freq.unsqueeze(0) # [S, H/2]\n    angles = torch.cat([angles, angles],dim=-1) # [S, H]\n    \n    cos = torch.cos(angles) # [S, H]\n    sin = torch.sin(angles) # [S, H]\n    \n    \n    return cos, sin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.271022Z","iopub.execute_input":"2024-11-17T18:22:49.271516Z","iopub.status.idle":"2024-11-17T18:22:49.280443Z","shell.execute_reply.started":"2024-11-17T18:22:49.271463Z","shell.execute_reply":"2024-11-17T18:22:49.278944Z"}},"outputs":[],"execution_count":7},{"id":"e7f2e1c8","cell_type":"code","source":"def apply_rope(x, cos, sin):\n    B, nH, S, H = x.shape\n    x1 = x[...,:H//2] # [B, nH, S, H/2]\n    x2 = x[...,H//2:] # [B, nH, S, H/2]\n    cos_values = cos[:S,:].unsqueeze(0).unsqueeze(1) # [1,1,S,H]\n    sin_values = sin[:S,:].unsqueeze(0).unsqueeze(1) # [1,1,S,H]\n    rotated = torch.cat([-x2,x1],dim=-1)\n    x_rope = (x * cos_values) + (rotated * sin_values)\n    return x_rope.to(x.dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.281813Z","iopub.execute_input":"2024-11-17T18:22:49.282246Z","iopub.status.idle":"2024-11-17T18:22:49.295289Z","shell.execute_reply.started":"2024-11-17T18:22:49.282205Z","shell.execute_reply":"2024-11-17T18:22:49.294091Z"}},"outputs":[],"execution_count":8},{"id":"9ac6dc04","cell_type":"code","source":"head_dim = 512\ninv_freq2 = 1.0 / (10000 ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n# plt.plot(inv_freq2.numpy())\n# plt.show()\nprint(torch.arange(256)[None,:].shape ,inv_freq2[:,None].shape, (torch.arange(256)[None,:] * inv_freq2[:,None]).shape)\ncos, sin = precompute_rope(head_dim)\nprint(cos.shape, sin.shape)\n\n\n# plot it to make sure\n# import numpy\n# import matplotlib.pyplot as plt\n# plt.figure(figsize=(12,36))\n# plt.imshow(cos.numpy())\n# plt.show()\n# plt.figure(figsize=(12,36))\n# plt.imshow(sin.numpy())\n# plt.show()\n\nx = torch.rand(2,8,128,64)\ncos,sin = precompute_rope(64,context_length=128)\nx_rope = apply_rope(x, cos, sin)\nx_rope.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.296820Z","iopub.execute_input":"2024-11-17T18:22:49.297506Z","iopub.status.idle":"2024-11-17T18:22:49.373791Z","shell.execute_reply.started":"2024-11-17T18:22:49.297464Z","shell.execute_reply":"2024-11-17T18:22:49.372624Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 256]) torch.Size([256, 1]) torch.Size([256, 256])\ntorch.Size([4096, 512]) torch.Size([4096, 512])\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 8, 128, 64])"},"metadata":{}}],"execution_count":9},{"id":"1eea3c44","cell_type":"code","source":"class GroupedQueryAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.embed_dim = config.embed_dim\n        self.num_kv_heads = config.num_kv_heads\n        self.num_q_heads = config.num_q_heads\n        \n        assert self.embed_dim % self.num_q_heads == 0, 'embed_dim should be div. by num. of query heads'\n        assert self.num_q_heads % self.num_kv_heads ==0, 'num. query heads should be div. by num. key-value heads'\n        \n        self.head_dim = self.embed_dim // self.num_q_heads\n        \n        self.q_proj = nn.Linear(self.embed_dim, self.head_dim * self.num_q_heads, bias=False, dtype=config.dtype)\n        self.k_proj = nn.Linear(self.embed_dim, self.head_dim * self.num_kv_heads, bias=False, dtype=config.dtype)\n        self.v_proj = nn.Linear(self.embed_dim, self.head_dim * self.num_kv_heads, bias=False, dtype=config.dtype)\n        \n        self.drop = nn.Dropout(config.attn_dropout)\n        \n        self.o_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False, dtype=config.dtype)\n        \n        self.register_buffer('causal_mask',\n                             torch.triu(torch.ones(\n                                 config.max_position_embeddings,\n                                 config.max_position_embeddings\n                             ),diagonal=1))\n        \n        cos, sin = precompute_rope(self.head_dim,base_theta=config.base_theta,context_length=config.max_position_embeddings)\n        self.register_buffer('rope_cos', cos)\n        self.register_buffer('rope_sin', sin)\n        \n    def forward(self, x):\n        # x [B S D]\n        \n        B,S,D = x.shape\n        \n        q = self.q_proj(x) # [B S H*nQ]\n        k = self.k_proj(x) # [B S H*nKV]\n        v = self.v_proj(x) # [B S H*nKV]\n        \n        q = q.view(B, S, self.num_q_heads, self.head_dim).transpose(1,2) # [B nQ S H]\n        k = k.view(B, S, self.num_kv_heads, self.head_dim).transpose(1,2) # [B nKV S H]\n        v = v.view(B, S, self.num_kv_heads, self.head_dim).transpose(1,2) # [B nKV S H]\n        \n        q = apply_rope(q, self.rope_cos, self.rope_sin)\n        k = apply_rope(k, self.rope_cos, self.rope_sin)\n        \n        k = k.repeat_interleave(self.num_q_heads//self.num_kv_heads, dim=1) # [B nQ S H]\n        v = v.repeat_interleave(self.num_q_heads//self.num_kv_heads, dim=1) # [B nQ S H]\n        \n        attn = q @ k.transpose(2,3) # [B nQ S H] @ [B nQ H S] = [B nQ S S]\n        \n        # apply mask, mul with v, reshape, return\n        mask = self.causal_mask[:S,:S].bool()\n        attn.masked_fill_(mask,-torch.inf)\n        \n        attn = F.softmax(attn / (self.head_dim ** 0.5), dim=-1)\n        \n        attn = self.drop(attn)\n        \n        out = attn @ v # [B nQ S S] @ [B nQ S H] = [B nQ S H]\n        out = out.transpose(1,2) # [B S nQ H]\n        out = out.reshape(B, S, D)\n        \n        proj = self.o_proj(out)\n        \n        return proj","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.378759Z","iopub.execute_input":"2024-11-17T18:22:49.379305Z","iopub.status.idle":"2024-11-17T18:22:49.403469Z","shell.execute_reply.started":"2024-11-17T18:22:49.379247Z","shell.execute_reply":"2024-11-17T18:22:49.402066Z"}},"outputs":[],"execution_count":10},{"id":"a7bbebed","cell_type":"code","source":"class LlamaDecoderLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        \n        self.self_attn = GroupedQueryAttention(config)\n        self.mlp = LlamaMLP(config)\n        \n        self.input_layernorm = LlamaRMSNorm(config.embed_dim)\n        self.post_attention_layernorm = LlamaRMSNorm(config.embed_dim)\n        \n        \n    def forward(self, x):\n        # x [B S D]\n        skip = x\n        x = self.input_layernorm(x)\n        x = self.self_attn(x)\n        x = x + skip\n        \n        skip = x\n        x = self.post_attention_layernorm(x)\n        x = self.mlp(x)\n        x = x + skip\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.404974Z","iopub.execute_input":"2024-11-17T18:22:49.405477Z","iopub.status.idle":"2024-11-17T18:22:49.417675Z","shell.execute_reply.started":"2024-11-17T18:22:49.405424Z","shell.execute_reply":"2024-11-17T18:22:49.416477Z"}},"outputs":[],"execution_count":11},{"id":"c02261a0","cell_type":"code","source":"class LLaMA(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.embed_tokens = nn.Embedding(\n            self.config.vocab_size, \n            self.config.embed_dim, \n            padding_idx=self.config.eos_token_id,\n            dtype=self.config.dtype)\n        self.layers = nn.ModuleList([\n            LlamaDecoderLayer(self.config) for _ in range(self.config.num_layers)\n        ])\n\n        self.norm = LlamaRMSNorm(self.config.embed_dim)\n        self.lm_head = nn.Linear(self.config.embed_dim, self.config.vocab_size, bias=False, dtype=self.config.dtype)\n\n        # weight tying\n        self.embed_tokens.weight = self.lm_head.weight\n        \n    def forward(self, input_ids):\n        # input_ids [B S]\n        x = self.embed_tokens(input_ids)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.norm(x)\n        logits = self.lm_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.419292Z","iopub.execute_input":"2024-11-17T18:22:49.419683Z","iopub.status.idle":"2024-11-17T18:22:49.430360Z","shell.execute_reply.started":"2024-11-17T18:22:49.419641Z","shell.execute_reply":"2024-11-17T18:22:49.429204Z"}},"outputs":[],"execution_count":12},{"id":"3882e0f2","cell_type":"code","source":"config = SimpleNamespace(\n    embed_dim = 576,\n    intermediate_dim = 1536,\n    max_position_embeddings = 8192,\n    base_theta = 100000,\n    num_q_heads = 9,\n    num_kv_heads = 3,\n    attn_dropout = 0.,\n    num_layers = 30,\n    vocab_size = 49152,\n    eos_token_id = 2,\n    dtype = torch.float32,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.432189Z","iopub.execute_input":"2024-11-17T18:22:49.432747Z","iopub.status.idle":"2024-11-17T18:22:49.443329Z","shell.execute_reply.started":"2024-11-17T18:22:49.432694Z","shell.execute_reply":"2024-11-17T18:22:49.442171Z"}},"outputs":[],"execution_count":13},{"id":"954ed46b","cell_type":"code","source":"if 'model' in globals() or 'model' in locals():\n    print('...')\n    del model\nmodel = LLaMA(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:22:49.444826Z","iopub.execute_input":"2024-11-17T18:22:49.445328Z","iopub.status.idle":"2024-11-17T18:23:04.412890Z","shell.execute_reply.started":"2024-11-17T18:22:49.445287Z","shell.execute_reply":"2024-11-17T18:23:04.411758Z"}},"outputs":[],"execution_count":14},{"id":"2e7a2144","cell_type":"code","source":"def model_memory_size(model, input_dtype=torch.float32):\n    total_params = 0\n    total_grads = 0\n    for param in model.parameters():\n        # Calculate total number of elements per parameter\n        param_size = param.numel()\n        total_params += param_size\n        # Check if gradients are stored for this parameter\n        if param.requires_grad:\n            total_grads += param_size\n\n    # Calculate buffer size (non-parameters that require memory)\n    total_buffers = sum(buf.numel() for buf in model.buffers())\n\n    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n    # We assume parameters and gradients are stored in the same type as input dtype\n    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n\n    # Convert bytes to gigabytes\n    total_memory_gb = total_memory_bytes / (1024**3)\n\n    return total_memory_gb, total_params\n\ntotal_mem, total_params = model_memory_size(model, input_dtype=torch.float32)\nprint(f\"float32 (PyTorch default): {total_mem:.2f} GB with {total_params:,} parameters\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:23:04.414310Z","iopub.execute_input":"2024-11-17T18:23:04.414752Z","iopub.status.idle":"2024-11-17T18:23:04.429069Z","shell.execute_reply.started":"2024-11-17T18:23:04.414703Z","shell.execute_reply":"2024-11-17T18:23:04.427912Z"}},"outputs":[{"name":"stdout","text":"float32 (PyTorch default): 8.62 GB with 134,515,008 parameters\n","output_type":"stream"}],"execution_count":15},{"id":"d5b6c75b","cell_type":"code","source":"x = torch.randint(0,config.vocab_size,(1,10)).long()\nx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:23:04.430465Z","iopub.execute_input":"2024-11-17T18:23:04.430823Z","iopub.status.idle":"2024-11-17T18:23:04.452179Z","shell.execute_reply.started":"2024-11-17T18:23:04.430778Z","shell.execute_reply":"2024-11-17T18:23:04.450872Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor([[20857, 34648, 24865, 15421, 38794, 18933, 40766, 17794, 29075, 34495]])"},"metadata":{}}],"execution_count":16},{"id":"49644e26","cell_type":"code","source":"out = model(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:23:04.453418Z","iopub.execute_input":"2024-11-17T18:23:04.453753Z","iopub.status.idle":"2024-11-17T18:23:04.596668Z","shell.execute_reply.started":"2024-11-17T18:23:04.453717Z","shell.execute_reply":"2024-11-17T18:23:04.595443Z"}},"outputs":[],"execution_count":17},{"id":"9413d424","cell_type":"code","source":"out.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:23:04.598274Z","iopub.execute_input":"2024-11-17T18:23:04.599026Z","iopub.status.idle":"2024-11-17T18:23:04.606357Z","shell.execute_reply.started":"2024-11-17T18:23:04.598971Z","shell.execute_reply":"2024-11-17T18:23:04.605204Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 10, 49152])"},"metadata":{}}],"execution_count":18},{"id":"ad3614ac-c5e5-42bd-bd73-d17b86e5a92f","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\nsmol = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:23:04.608146Z","iopub.execute_input":"2024-11-17T18:23:04.608752Z","iopub.status.idle":"2024-11-17T18:23:16.220965Z","shell.execute_reply.started":"2024-11-17T18:23:04.608699Z","shell.execute_reply":"2024-11-17T18:23:16.219579Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1afe684e20d4fc9a250dc87b9285718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f7c698cf3b14ebb844a57080bab50e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a31c823f4774b9bab3134ca75d9c672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ba4aa1d12814190b528f8eaeaffa39c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"755a379d4a5645f781f3cf8c95da4244"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"324c180a2061405eb1f943f26fe3f050"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67f6f8b10cc94eec9d47782419a890cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfc13ae5d26a4f48adb39ad3e399f989"}},"metadata":{}}],"execution_count":19},{"id":"4f2315d0-c56d-48b8-becc-93793a4aa809","cell_type":"code","source":"smol_sd = smol.state_dict()\nmodel_sd = model.state_dict()\nmodel_sd = {k:v for k,v in model_sd.items() if not any([s in k for s in ['rope','causal_mask']])}\n\nlen(smol_sd), len(model_sd)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:23:16.222845Z","iopub.execute_input":"2024-11-17T18:23:16.223422Z","iopub.status.idle":"2024-11-17T18:23:16.243995Z","shell.execute_reply.started":"2024-11-17T18:23:16.223376Z","shell.execute_reply":"2024-11-17T18:23:16.242772Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(273, 273)"},"metadata":{}}],"execution_count":20},{"id":"36be9788-74b0-41bf-916b-3730a43754fa","cell_type":"code","source":"sum([p.numel() for p in smol.parameters()]),sum([p.numel() for p in model.parameters()])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:23:16.245371Z","iopub.execute_input":"2024-11-17T18:23:16.245716Z","iopub.status.idle":"2024-11-17T18:23:16.329817Z","shell.execute_reply.started":"2024-11-17T18:23:16.245679Z","shell.execute_reply":"2024-11-17T18:23:16.328494Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(134515008, 134515008)"},"metadata":{}}],"execution_count":21},{"id":"78231b70-ab3b-448b-bd47-5f099e20fcf6","cell_type":"code","source":"keys = list(model_sd.keys())\nfor idx,(k,v) in enumerate(smol_sd.items()):\n    new_key = keys[idx]\n    setattr(model,new_key,v.clone())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:37:51.624729Z","iopub.execute_input":"2024-11-17T18:37:51.625346Z","iopub.status.idle":"2024-11-17T18:37:52.084772Z","shell.execute_reply.started":"2024-11-17T18:37:51.625286Z","shell.execute_reply":"2024-11-17T18:37:52.083429Z"}},"outputs":[],"execution_count":29},{"id":"16d41db5-ee66-4097-ba9b-833538cb6cbd","cell_type":"code","source":"torch.allclose(smol.model.embed_tokens.weight, model.embed_tokens.weight)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:38:37.698378Z","iopub.execute_input":"2024-11-17T18:38:37.699253Z","iopub.status.idle":"2024-11-17T18:38:38.428354Z","shell.execute_reply.started":"2024-11-17T18:38:37.699186Z","shell.execute_reply":"2024-11-17T18:38:38.427087Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":34},{"id":"fc1e23a6-2565-406d-b537-e45edd3bb731","cell_type":"code","source":"def get_input(text):\n    messages = [{\"role\": \"user\", \"content\": text}]\n    input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T18:47:32.397629Z","iopub.execute_input":"2024-11-17T18:47:32.398131Z","iopub.status.idle":"2024-11-17T18:47:32.406526Z","shell.execute_reply.started":"2024-11-17T18:47:32.398067Z","shell.execute_reply":"2024-11-17T18:47:32.405033Z"}},"outputs":[],"execution_count":40},{"id":"9c946100-2771-41a0-9509-70ca4b2e5476","cell_type":"code","source":"def generate(\n    model,\n    input_ids,\n    max_new_tokens=32,\n    context_length = config.max_position_embeddings,\n    temperature = 0.,\n    eos_token_id = config.eos_token_id\n):\n    model.eval()\n    inputs = input_ids.clone()\n    print(tokenizer.decode(inputs.flatten().numpy()))\n    for _ in range(max_new_tokens):\n        context = inputs[:,-context_length:]\n        with torch.inference_mode():\n            logits = model(context)\n            logits = logits[:,-1,:]\n\n            if temperature > 0.:\n                logits = logits / temperature\n\n            probs = logits.softmax(dim=-1)\n\n            if temperature > 0.:\n                next_token = torch.multinomial(probs, num_samples=1)\n            else:\n                next_token = torch.argmax(probs,dim=-1,keepdim=True)\n\n            if next_token == eos_token_id:\n                break\n        print(tokenizer.decode(next_token.flatten().numpy()),end='')\n        inputs = torch.cat([inputs, next_token],dim=1)  \n    print()\n    return inputs            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:03:07.204806Z","iopub.execute_input":"2024-11-17T19:03:07.205320Z","iopub.status.idle":"2024-11-17T19:03:07.218688Z","shell.execute_reply.started":"2024-11-17T19:03:07.205275Z","shell.execute_reply":"2024-11-17T19:03:07.217382Z"}},"outputs":[],"execution_count":64},{"id":"f9f8216e-9d7a-4771-98d9-f7b2a95ee5ad","cell_type":"code","source":"input = get_input('give me a random fact about llamas')\ngenerated = generate(model, input, max_new_tokens=80, temperature=0.125)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T19:07:22.399289Z","iopub.execute_input":"2024-11-17T19:07:22.399937Z","iopub.status.idle":"2024-11-17T19:07:45.959797Z","shell.execute_reply.started":"2024-11-17T19:07:22.399882Z","shell.execute_reply":"2024-11-17T19:07:45.958574Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n<|im_start|>user\ngive me a random fact about llamas<|im_end|>\n\n<|im_start|>assistant\nLlamas are large, four-legged animals native to the Andes Mountains of South America. They are known for their unique adaptations, including their ability to run at incredible speeds of up to 30 miles per hour. They are also known for their distinctive horns, which are shaped like a pair of horns, and their ability to climb trees. Llamas are also known\n","output_type":"stream"}],"execution_count":68}]}